\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{float}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs} % For prettier tables
\usepackage{siunitx}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{natbib}  % Or another suitable package for bibliography management

\DeclareMathOperator*{\argmin}{arg\,min}  % The asterisk ensures that subscripts are positioned correctly in display style.

% Set page margins
\geometry{a4paper, margin=1in}

% Set up code listing style
\lstset{
    basicstyle=\ttfamily,
    commentstyle=\colour{gray},
    keywordstyle=\colour{blue},
    stringstyle=\colour{red},
    showstringspaces=false,
    captionpos=b
}
\begin{document}
\title{Image Analysis Coursework}
\author{Vishal Jain}
\date{\today}
\newpage
\maketitle
Word Count (Excluding this and the appendix): 2982
\tableofcontents
\newpage
\section{Question 1 - Image Segmentation}
\subsection{Part A: Lung CT Image Segmentation}
For the segmentation of the lung CT image, shown in Figure \ref{fig:lung_ct}, the primary objective was to accurately delineate the lung tissue, including nodules, from surrounding anatomical structures. The inherent high contrast between the lung fields and adjacent tissues suggests the utility of a threshold-based segmentation method.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{../data/CT.png}
    \caption{Lung CT image.}
    \label{fig:lung_ct}
\end{figure}

\subsubsection{Segmentation Algorithm:}

\begin{itemize}
    \item \textbf{Otsu Thresholding:} The image is binarised via Otsu's threshold, which finds the threshold that maximises the inter-class variance.
    \item \textbf{Connected Component Analysis:} Connected component analysis with 8-connectivity is used to select the left and right lungs.
    \item \textbf{Binary Hole Filling:} To recover the nodules and tissue within the lung regions, binary hole filling is performed.
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1a_binary.png}  % Replace with your image file
        \caption{}
        \label{fig:otsu_threshold}
    \end{subfigure}%
    \begin{subfigure}{.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1a_largest_connected_components.png}  % Replace with your image file
        \caption{}
        \label{fig:connected_components}
    \end{subfigure}%
    \begin{subfigure}{.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1a_filled_holes.png}  % Replace with your image file
        \caption{}
        \label{fig:filled_holes}
    \end{subfigure}
    \label{fig:segmentation_steps}
\caption{Segmentation Algorithm: (a) Binarisation using Otsu's threshold, (b) Connected component analysis, (c) Binary hole filling.}
\end{figure}

Note, this segmentation algorithm was also implemented from scratch. Relevant code can be found in \texttt{src/q1a\_from\_scratch.py} and \texttt{src/from\_scratch\_seg\_funcs.py}.

\subsubsection{Results}
The final segmentation of the lung CT image is shown in Figure \ref{fig:lung_ct_segmentation} The segmentation clearly captures the relevant lung regions and all the nodules and tissues within.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/q1a.png}
    \caption{Final segmentation of the lung CT image.}
    \label{fig:lung_ct_segmentation}
\end{figure}

\subsubsection{Discussion}
Figure \ref{fig:q1a_ambiguity} shows a potential ambiguous region in the segmentation. This region was segmented due to having a similar intensity as the lung tissue and being connected to lung tissue post Otsu thresholding. However, as this region could not be clearly identified as non lung tissue without additional context, it was kept in the final segmentation.
\begin{figure}[H]
    \centering
    \begin{subfigure}{.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1a_seg_w_bbox.png}  % Replace with your image file
        \caption{}
        \label{fig:q1a_seg_w_bbox}
    \end{subfigure}%
    \begin{subfigure}{.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1a_zoomed_region.png}  % Replace with your image file
        \caption{}
        \label{fig:q1a_zoomed_region}
    \end{subfigure}%
    \caption{Ambiguous region in the segmentation: (a) Segmentation with bounding box around the ambiguous region, (b) Zoomed in view of the ambiguous region.}
    \label{fig:q1a_ambiguity}
\end{figure}

\subsection{Part B: Noisy Flowers Image Segmentation}
This section details the segmentation algorithm of the noisy flowers image shown in Figure \ref{fig:flowers_image}. The region to be segmented are all the purple flower heads in the image.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../data/noisy_flower.jpg}
    \caption{Noisy Flowers image.}
    \label{fig:flowers_image}
\end{figure}

The noise in the image  was classified to be gaussian noise due to the success of applying gaussian filters seperately on each colour channel, shown in Figure \ref{fig:gaussian_noise_evidence}.
\begin{figure}[H]
    \centering
    \begin{subfigure}{.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1b_patch.png}  % Replace with your image file
        \caption{}
        \label{fig:flower_patch}
    \end{subfigure}%
    \begin{subfigure}{.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1b_gauss_blurred_patch.png}  % Replace with your image file
        \caption{}
        \label{fig:gaussian_filtered_flower_patch}
    \end{subfigure}%
    \caption{Effect of Gaussian filtering on the colour channels of the noisy flowers image: (a) Cropped region of the noisy flowers image, (b) Cropped region after applying a Gaussian filter with standard deviation 1.}
    \label{fig:gaussian_noise_evidence}
\end{figure}

\subsubsection{Segmentation Algorithm:}

\begin{itemize}
    \item \textbf{Bilateral Gaussian Filtering:} Initial denoising is achieved using a bilateral filter, chosen for its ability to reduce noise while preserving edges by considering both color similarity and spatial proximity. This step maintains the integrity of flower boundaries against the noise.
    
    \item \textbf{Conversion to LAB Color Space:} The image is converted from RGB to LAB color space. Euclidean distances in LAB more closely resemble perceptual differences and thus makes the clustering more effective.
    
    \item \textbf{K-Means Clustering:} K means is applied with five clusters to match the number of distinct color groups observed.
    
    \item \textbf{Post-Clustering Processing:} Additional steps include selecting the cluster closest to purple's LAB value and refining the segmentation mask through morphological operations and removing small connected components.
\end{itemize}
\begin{figure}[H]
    \centering
    \begin{subfigure}{.45\textwidth}  % Adjusted to fit two figures per row
        \centering
        \includegraphics[width=\linewidth]{figs/q1b_denoised.png}
        \caption{}
        \label{fig:denoised_flowers}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}  % Adjusted to fit two figures per row
        \centering
        \includegraphics[width=\linewidth]{figs/q1b_kmeans_mask.png}
        \caption{}
        \label{fig:kmeans_mask_flowers}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}  % New row, first column
        \centering
        \includegraphics[width=\linewidth]{figs/q1b_kmeans_mask_post_opening.png}
        \caption{}
        \label{fig:kmeans_mask_post_opening_flowers}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}  % New row, second column
        \centering
        \includegraphics[width=\linewidth]{figs/q1b_mask_post_cnc_removal.png}
        \caption{}
        \label{fig:cnc_threshold_flowers}
    \end{subfigure}%
    \caption{Segmentation algorithm: (a) Bilateral filtering (b) K-means clustering to segment purple pixels. (c) Segmentation mask post morphological opening (d) Final segmentation after removing small connected components}
    \label{fig:q1b_segmentation_steps}
\end{figure}

\subsubsection{Results}
The final segmentation of the noisy flowers image is shown in Figure \ref{fig:q1b_final_mask}. The segmentation clearly captures most of the purple flower heads in the main central bunch. It also captures some in the smaller bunch in the upper left corner. It is able to distinguish between the colours effectively, with some of the orange and red flowers in the purple group being excluded from the segmentation quite effectively as seen in figure \ref{fig:flowers_seg_zoomed}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/q1b_final_mask.png}
    \caption{Final segmentation of the noisy flowers image.}
    \label{fig:q1b_final_mask}
\end{figure}

% TODO: Think of a better colouring scheme for the bounding boxes, white is washing out the image.
\begin{figure}[H]
    \centering
    \begin{subfigure}[c]{.6\textwidth}  % Centered subfigure on the left
        \centering
        \includegraphics[width=\linewidth]{figs/q1b_seg_w_good_bbox.png}
        \caption{}  % Caption for the left subfigure
        \label{fig:q1b_zoomed}
    \end{subfigure}%
    \begin{minipage}[c]{.35\textwidth}  % Centered minipage for the right subfigures
        \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figs/q1b_zoomed_region1.png}
            \caption{}  % Caption for the upper right subfigure
            \label{fig:q1b_zoomed_bad}
        \end{subfigure}\\[1ex]  % Add some space between the subfigures
        \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figs/q1b_zoomed_region2.png}
            \caption{}  % Caption for the lower right subfigure
            \label{fig:q1b_zoomed_bad2}
        \end{subfigure}
    \end{minipage}
    \caption{Zoomed in view of the segmentation: (a) Segmentation with bounding boxes around regions of interest, (b) Correct exclusion of orange flower from the purple group, (c) Correct exclusion of orange and red flowers that are occluded by flowers from the purple group.}
    \label{fig:flowers_seg_zoomed}
\end{figure}

\subsubsection{Discussion}
The primary challenge in segmenting the foreground purple flower heads arises from shadows that darken the lower half, complicating color-based segmentation. This issue is illustrated in Figure \ref{fig:q1b_seg_lim}.
\begin{figure}[H]
    \centering
    \begin{subfigure}{.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1b_seg_w_bad_bbox.png}
        \caption{}  % Optional: add a specific caption here if needed.
        \label{fig:q1b_bad_seg_w_bbox}
    \end{subfigure}\\ % Add a line break to stack the next subfigure below
    \begin{subfigure}{.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1b_bad_zoomed_region.png}
        \caption{}  % Optional: add a specific caption here if needed.
        \label{fig:q1b_bad_zoom}
    \end{subfigure}
    \caption{Limitations in segmentation: (a) Full view of the image with poorly segmented areas highlighted in a bounding box (b) Flowers with strong shadows}
    \label{fig:q1b_seg_lim}
\end{figure}

\subsection{Part C: Coin Image Segmentation}
This section details the segmentation algorithm of the coin image shown in Figure \ref{fig:coins_image}. The region to be segmented are the first coin in the first row, the second coin in the second row, the third coin in the third row and the fourth coin in the fourth row. The image has been degraded with vertical line type artefacts, however can be corrected by filtering. There is generally good contrast between the coins and the background suggesting that edge detection can be used to segment the coins.
\begin{figure}[H] 
    \centering
    \includegraphics[width=0.5\textwidth]{../data/coins.png}
    \caption{Coin image.}
    \label{fig:coins_image}
\end{figure}



\subsubsection{Segmentation Algorithm}
\begin{itemize}
\item \textbf{Row-wise Median Filtering:} A row-wise median filter is applied to the image to remove vertical line artefacts.
\item \textbf{Edge Detection with Canny:} Canny edge detection is used to get a binary edge mask. Canny edge detection is a multi-stage algorithm that uses Gaussian smoothing to reduce noise, then detects edges by identifying areas with high gradient magnitudes. It employs a double threshold to classify edges into strong, weak, and non-edges: strong edges exceed the high threshold and are confirmed as true edges; weak edges between the two thresholds are only confirmed if connected to strong edges; and areas below the low threshold are dismissed as non-edges.
\item \textbf{Dilation of Edges:} Next, a dilation operation is performed to close small gaps in the edge outlines of the coins.
\item \textbf{Removal of the Largest Component:} The resulting image is then labelled using 4-connectivity. The largest connected component is assumed to be the background and is removed.
\item \textbf{Integration of Edges and Mask:} Edges detected from the Canny step are added back to enhance the definitions of the coins' borders.
\item \textbf{Binary Hole Filling:} A binary hole filling operation is then performed to fill any unsegmented areas within each coin. 
\item \textbf{Median Filtering on Mask:} Finally, to remove spurious regions of the mask which arose from edges that did not belong to any coins, both row-wise and column-wise median filtering is applied to the mask. This step ensures that the next step of removing small components need not require too large a threshold, risking the removal of actual coins.
\item \textbf{Removal of Small Connected Components:} Finally, any small connected components with an area less than 100 pixels are removed from the mask.
\item \textbf{Sorting and Assigning Grid Positions:} The centroids of the coin masks are then sorted and used to assign each coin a grid position. 
\item \textbf{Selecting Specific Coins:} The coins whose grid position matches the required positions are selected.

\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1c_init_median_filtered.png}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1c_canny_edges.png}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1c_dilated_inverted_edges.png}
        \caption{}
    \end{subfigure}%

    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1c_connected_components.png}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1c_mask_no_bg.png}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1c_mask_edges.png}
        \caption{}
    \end{subfigure}%

    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1c_binary_hole_filled.png}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1c_median_filtered_mask.png}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1c_mask_no_small_components.png}
        \caption{}
    \end{subfigure}%

    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1c_identified_coins.png}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q1c_selected_coins.png}
        \caption{}
    \end{subfigure}%
    
    \caption{Segmentation Algorithm: (a) Initial row wise median filtering (b) Canny edge detection (c) Dilated edges (d) Identification of connected components (e) Binary Mask after background removal (f) Mask integrated with edges, (g) Binary hole filling, (h) Post row-wise and column-wise median filtered mask (i) Final segmentation after eliminating small connected components, (j) Identified coins with their grid positions, (k) Final Selected coins.}
    \label{fig:coin_segmentation_steps}
\end{figure}

\subsubsection{Results}
The final segmentation of the coin image is shown in Figure \ref{fig:q1c_selected_coins}. The segmentation clearly captures the required coins in the specified grid positions.  

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/q1c.png}
    \caption{Final segmentation of the coin image.}
    \label{fig:q1c_selected_coins}
\end{figure}

\subsubsection{Discussion}
The chosen method works almost perfectly, the only issue is that the median filtering and edge dilation steps can result in the final mask being slightly unfaithful to the actual coin shapes around the edge.
\section{Question 2 - Inverse Problems and Multi-resolution analysis}
\subsection{Part A - Solving Over Determined Inverse Problems: Line Fitting}
In this scenario, the goal is to fit a line described by the equation \( y = ax + c \) to a dataset consisting of noisy measurements. This is a typical example of a linear inverse problem where we aim to determine the parameters of a line, which in mathematical terms can be framed as:
\[
y = Ax + b.
\]
Here:
\begin{itemize}
    \item \(y\) is the vector of observed \(y\)-coordinates from the dataset, representing the outputs or dependent variables.
    \item \(A\) is the system matrix that incorporates the independent variable data. For each measurement pair \((x_i, y_i)\), the matrix \(A\) contains a row \([x_i, 1]\) to account for the slope and the intercept of the line.
    \item \(x\) is the vector of parameters to be estimated, specifically \([a, c]\), where \(a\) is the slope and \(c\) is the intercept of the line.
    \item \(b\) represents the vector of measurement errors.
\end{itemize}
This linear regression problem is tackled for two datasets describing noisy line measurements. Both datasets are depicted in Figure \ref{fig:line_fitting_data}, illustrating the points used for line fitting. It is worth noting that the second dataset is identical to the first for all points except for one which is modified to describe an outlier point. 

This system is over-determined for the given datasets because there are more measurements (\(20\) pairs of \(x, y\) values) than there are parameters to estimate (just two, \(a\) and \(c\)). Although over-determined systems may be well posed and have a unique solution and when all measurements are consistent with the true signal. However, in the presence of noise, the system will not have any exact solutions, where all data points lie perfectly on the same line. Instead, the objective is to find the best estimates for the parameters \(x = [a, c]\) that minimise the difference between the predicted line \( Ax \) and the observed data \( y \), via the objective function. In this problem, the \(L_2\) and \(L_1\) norms of the error vector, $\epsilon = y - Ax$, are explored as the objective functions. 


\begin{figure}[H]
    \centering
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2a_noisy_line.png}
        \caption{}
        \label{fig:line_data_1}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2a_noisy_line_outlier.png}
        \caption{}
        \label{fig:line_data_2}
    \end{subfigure}%
    \caption{Two datasets describing noisy measurements of points along a line: (a) Dataset 1 - line data with noisy observations (b) Dataset 2 - line data with noisy observations and a single outlier.}
    \label{fig:line_fitting_data}
\end{figure}
% Describe the method used to solve the problem in 2a
\subsubsection{Results}
To minimise the objective function for the given datasets, the \texttt{minimize} function from the \texttt{scipy.optimize} library is employed. This function uses the SLSQP optimisation algorithm to find the minimum of the objective function \cite{Kraft1988}. The results of the line fitting for the two datasets are shown in Figure \ref{fig:line_fitting_results} and the estimated line parameters are summarised in Table \ref{tab:line_fitting_results}.

\begin{figure}[H]
    \centering
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2a_fitted_line_1.png}
        \caption{}
        \label{fig:line_fit_1}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2a_fitted_line_2.png}
        \caption{}
        \label{fig:line_fit_2}
    \end{subfigure}%
    \caption{Results of line fitting using the SLSQP optimization algorithm to minimise the \(L_2\) and \(L_1\) error norms for the two datasets, which are shown in green and orange respectively: (a) Dataset 1, (b) Dataset 2 with the outlier.}
    \label{fig:line_fitting_results}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{@{}llll@{}}
    \toprule
    \textbf{Dataset} & \textbf{Objective Function} & \textbf{Slope (a)} & \textbf{Intercept (c)} \\ \midrule
    1               & \(L_2\) Norm                & 0.0665             & 0.2172                 \\
                    & \(L_1\) Norm                & 0.0663             & 0.2429                 \\ \midrule
    2               & \(L_2\) Norm                & 0.0462             & 0.5804                 \\
                    & \(L_1\) Norm                & 0.0662             & 0.2458                 \\ \bottomrule
    \end{tabular}
    \caption{Summary of the estimated line parameters for the two datasets using the \(L_2\) and \(L_1\) error norms.}
    \label{tab:line_fitting_results}
\end{table}

\subsubsection{Discussion}
In the absence of outliers, both \(L_2\) and \(L_1\) norms provide comparable results for line fitting. However, when outliers are present, the \(L_1\) norm offers a more accurate estimation because it minimises the absolute errors, which reduces the impact of extreme values. Conversely, the \(L_2\) norm, which minimises the sum of squared errors, gives disproportionately high weight to outliers. This sensitivity causes the \(L_2\) norm to be more influenced by the outlier, skewing the line fit significantly away from the majority of the data points, as observed in the results for the second dataset.



\subsection{Part B - Compressed Sensing Reconstruction and Underdetermined Inverse Problems}

% Introduce the problem
This section addresses signal reconstruction under compressed sensing, which operates on the principle that many signals are sparse in an appropriate basis. Specifically, a signal \( x \in \mathbb{R}^n \) is \( K \)-sparse if \( x = \Psi s \) where \( \Psi \) is a basis matrix and \( s \) has only \( K \) non-zero entries, with \( K \ll n \). Compressed sensing allows for signal reconstruction from fewer measurements than the Nyquist rate by utilising a non-uniform or random measurement matrix \( C \), and solving the underdetermined equation:
\[
\argmin_{s} \left\| s \right\|_1 \quad \text{subject to} \quad y = C \Psi s.
\]
The \( L_1 \) norm minimisation promotes sparsity in \( s \), aiding in accurate reconstruction.


\subsubsection{Problem Setup}
A practical demonstration involves a generating a 1D sparse signal \( s \) with 100 components and 10 non-zero coefficients. Zero mean Gaussian noise (standard deviation 0.05) is added to simulate realistic conditions. $s$ is transformed into the Fourier domain via the discrete fast Fourier transform (DFFT) to yield \( X \).  The sparse signal and its Fourier transform are illustrated in Figure \ref{fig:compressed_sensing_signal}, where \( \Psi \) represents the DFFT matrix.
\begin{figure} [H]
    \centering
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_original_sparse_signal_fft.png}
        \caption{Frequency domain}
        \label{fig:sparse_signal_fourier}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_original_sparse_signal.png}
        \caption{Time Domain}
        \label{fig:sparse_signal}
    \end{subfigure}%

    \caption{Generated signal expressed in sparse and non sparse domains: (a) Non sparse signal \( X\) in the fourier domain, (b) sparse signal \( s \) in the time domain.}
    \label{fig:compressed_sensing_signal}
\end{figure}

These signals can be seen in Figure \ref{fig:compressed_sensing_signal}. The signal in the non sparse measurement domain, \(X\), is then sub sampled under uniform and random strategies to obtain two sets of 32 measurements \( y \). These sub sampled signals are shown in Figure \ref{fig:compressed_sensing_sub_sampled_signals_combined} in both the time and frequency domains. Note that a correction factor is applied to the IDFFT of \(y\) to account for the sub sampling reducing the energy of the signal.

\begin{figure}[H]
    \centering
    % Uniform Sampling Subfigures
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_uniformly_subsampled_signal_fft.png}
        \caption{Uniform Sampling - Frequency Domain}
        \label{fig:uniform_subsampled_signal_fft}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_uniformly_subsampled_signal.png}
        \caption{Uniform Sampling - Time Domain}
        \label{fig:uniform_subsampled_signal}
    \end{subfigure}
    
    % Random Sampling Subfigures
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_randomly_subsampled_signal_fft.png}
        \caption{Random Sampling - Frequency Domain}
        \label{fig:random_subsampled_signal_fft}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_randomly_subsampled_signal.png}
        \caption{Random Sampling - Time Domain}
        \label{fig:random_subsampled_signal}
    \end{subfigure}
    
    \caption{Comparison of uniformly and randomly subsampled \( y \) in both the non-sparse and sparse domains. Top row: Uniform sampling in (a) Fourier domain and (b) time domain. Bottom row: Random sampling in (c) Fourier domain and (d) time domain.}
    \label{fig:compressed_sensing_sub_sampled_signals_combined}
\end{figure}

\subsubsection{Results}
Using iterative soft thresholding in the sparse domain and enforcing consistency in the measurement domain, the signal is reconstructed. The reconstructed signals under uniform and random sampling strategies are shown in Figure \ref{fig:full_comparison}.

\begin{figure}[H]
    \centering
    % Original Signal Subfigures
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_original_sparse_signal_fft.png}
        \caption{Original - Frequency}
        \label{fig:original_signal_fft}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_original_sparse_signal.png}
        \caption{Original - Time}
        \label{fig:original_signal}
    \end{subfigure}

    % Reconstructed Signal Subfigures
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_uniformly_recovered_signal_fft.png}
        \caption{Reconstructed (Uniform) - Frequency}
        \label{fig:uniform_reconstructed_signal_fft}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_uniformly_recovered_signal.png}
        \caption{Reconstructed (Uniform) - Time}
        \label{fig:uniform_reconstructed_signal}
    \end{subfigure}
    
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_randomly_recovered_signal_fft.png}
        \caption{Reconstructed (Random) - Frequency}
        \label{fig:random_reconstructed_signal_fft}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2b_randomly_recovered_signal.png}
        \caption{Reconstructed (Random) - Time }
        \label{fig:random_reconstructed_signal}
    \end{subfigure}
    
    \caption{Comparison of the original and reconstructed signals under uniform and random sampling strategies in both the non-sparse and sparse domains.}
    \label{fig:full_comparison}
\end{figure}
The superior performance of the random sampling strategy in reconstructing the original signal is supported by compressed sensing theory. Both random and uniform sampling strategies obtain 32 samples, which is inline with the measurement condition \(O(K \log(N/K))\)—calculated as 10 for this scenario. The uniform sampling's failure can instead be attributed primarily to two factors: its non-compliance with the incoherence condition, which is crucial for compressed sensing efficacy, and its likely violation of the Nyquist-Shannon sampling criterion.

\subsection{Part C - Compressed Reconstruction and Multi-resolution Analysis}
This question focuses on the reconstruction of the image shown in Fig \ref{fig:river_side}, using a subset of its wavelet coefficients. The motivation is to demonstrate how natural images can have sparse representations in another domain.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../data/river_side.jpeg}
    \caption{River side image.}
    \label{fig:river_side}
\end{figure}

\subsubsection{Discrete Wavelet Transform of the Image}
The image is first cropped to remove white space and then processed using the discrete wavelet transform (DWT) with Daubechies wavelets across three levels of frequency decomposition. The wavelet coefficients are then binary thresholded, retaining as `1` only those whose magnitude falls within the top 15\%, setting all others to `0`. These thresholded coefficients are displayed in Figure \ref{fig:wavelet_coefficients}, organised hierarchically: the upper-left quadrant contains the low-frequency components, while the other quadrants show higher frequency details in horizontal, vertical, and diagonal orientations. This display underscores the concentration of energy in the low-frequency components and the sparsity of activations in higher frequencies.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{figs/q2c_river_side_wavelet_transform.jpeg}
    \caption{Thresholded Daubechies wavelet transform of the river side image, showing the top 15\% coefficients for 3 levels of decomposition.}
    \label{fig:wavelet_coefficients}
\end{figure}

The image can be reconstructed using the inverse DWT. The original, reconstruction and difference images are shown in Fig \ref{fig:full_reconstruction}, the negligible mean squared error between the reconstruction and original image illustrates how near perfect reconstruction can be achieved from the discrete set of wavelet coefficients. 

\begin{figure}[H]
    \centering
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2c_river_side_original.jpeg}
        \caption{Original Image}
        \label{fig:original_image}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2c_river_side_reconstructed.jpeg}
        \caption{Reconstructed Image}
        \label{fig:reconstructed_image}
    \end{subfigure}%
    
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2c_river_side_difference.jpeg}
        \caption{Difference Image}
        \label{fig:difference_image}
    \end{subfigure}%
    \caption{Comparison of the original and reconstructed river side images, along with the difference image.}
    \label{fig:full_reconstruction}
\end{figure}


\subsubsection{Image Reconstruction with Selective Coefficient Retention}
The reconstructed images, displayed in Figure~\ref{fig:thresholded_reconstruction}, result from retaining varying percentages of the largest wavelet coefficients---20\%, 10\%, 5\%, and 2.5\%. A discrete wavelet transform with two levels of decomposition is employed to make the effect of  the coefficient retention more pronounced. It is apparent that the quality of reconstruction diminishes as fewer coefficients are retained, particularly affecting areas of the image rich in high-frequency details. Remarkably, even with only 10\% of the coefficients preserved, the image reconstruction remains a highly accurate depiction of the original, underscoring the inherent sparsity of natural images in the wavelet domain.

\begin{figure}[H]
    \centering
    \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2c_river_side_threshold_0_2.jpeg}
        \caption{Reconstruction with top 20\% of coefficients retained}
        \label{fig:reconstructed_20}
    \end{subfigure}
    
    \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2c_river_side_threshold_0_1.jpeg}
        \caption{Reconstruction with top 10\% of coefficients retained}
        \label{fig:reconstructed_10}
    \end{subfigure}
    
    \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2c_river_side_threshold_0_05.jpeg}
        \caption{Reconstruction with top 5\% of coefficients retained}
        \label{fig:reconstructed_5}
    \end{subfigure}
    
    \begin{subfigure}{.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/q2c_river_side_threshold_0_025.jpeg}
        \caption{Reconstruction with top 2.5\% of coefficients retained}
        \label{fig:reconstructed_2_5}
    \end{subfigure}
    
    \caption{Reconstruction of the river side image using varying levels of coefficient retention.}
    \label{fig:thresholded_reconstruction}
\end{figure}

\section{Question 3 - Solving inverse problems}
\subsection{Part A - Convergence Rate of Gradient Descent}
% Introduce problem objective - function, set up, goal
In this exercise, the function $ f : \mathbb{R}^2 \rightarrow \mathbb{R} $ is considered, defined as $ f(\mathbf{x}) = \frac{1}{2} x_1^2 + x_2^2 $. The task is to minimise the function using gradient descent, starting from the initial point $ \mathbf{x}_0 = \begin{bmatrix} 1 \\ 1 \end{bmatrix} $. The goal is to theoretically determine the number of iterations $ K $ required to achieve a specified accuracy, defined as $ f(x) - f(x_*)$ where $x_*$ is the optimal solution, less than $ 0.01 $. This prediction is then compared to computational results.

\subsubsection{Proving \(f \) is Convex}
First the function convexity is verified by considering the Hessian matrix of \( f \): 
\[
H_f(\mathbf{x}) = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix}
\]
This matrix is constant and has positive eigenvalues (1 and 2) for all \( \mathbf{x} \in  \mathbb{R}^2 \), indicating that \( f \) is convex over the entire domain.

\subsubsection{Identifying the Lipschitz Constant of the Gradient of \( f \)}

The Lipschitz constant of the gradient of \( f \) is determined by considering the function \( f \) with the following gradient:
\[
\nabla f(\mathbf{x}) = \begin{bmatrix} x_1 \\ 2x_2 \end{bmatrix}.
\]

The Lipschitz constant \( L \) is derived through the application of the definition of the Lipschitz continuity property of the gradient of \( f \):
\[
\| \nabla f(\mathbf{x}) - \nabla f(\mathbf{x'}) \|_2 \leq L \| \mathbf{x} - \mathbf{x'} \|_2,
\]
where \( \mathbf{x} \) and \( \mathbf{x'} \) represent any two points in the domain of \( f \). 

The norm of the gradient difference is calculated as follows:
\[
\| \nabla f(\mathbf{x}) - \nabla f(\mathbf{x'}) \|_2 = \left\| \begin{bmatrix} x_1 - x'_1 \\ 2x_2 - 2x'_2 \end{bmatrix} \right\|_2 = \sqrt{(x_1 - x'_1)^2 + 4(x_2 - x'_2)^2}.
\]

\[
\sqrt{(x_1 - x'_1)^2 + 4(x_2 - x'_2)^2} \leq \sqrt{4} \cdot \sqrt{(x_1 - x'_1)^2 + (x_2 - x'_2)^2} = 2 \cdot \|\mathbf{x} - \mathbf{x'}\|_2.
\]

To confirm rigorously that any constant \( L < 2 \) does not satisfy the Lipschitz continuity property, specific vectors \( \mathbf{x} \) and \( \mathbf{x'} \) are considered such that the inequality is challenged. Specifically, if \( \mathbf{x} = \begin{bmatrix} 0 \\ x_2 \end{bmatrix} \) and \( \mathbf{x'} = \begin{bmatrix} 0 \\ x'_2 \end{bmatrix} \) are selected, where \( x_2 \) and \( x'_2 \) are any real numbers, the following is observed:
\[
\| \nabla f(\mathbf{x}) - \nabla f(\mathbf{x'}) \|_2 = 2 |x_2 - x'_2| = 2 \|\mathbf{x} - \mathbf{x'}\|_2.
\]

\[
2 \|\mathbf{x} - \mathbf{x'}\|_2 > L \|\mathbf{x} - \mathbf{x'}\|_2 \quad \forall L < 2.
\]
Therefore, it is shown that for \( L < 2 \), the inequality does not hold.

\subsubsection{Finding the Optimal Solution}
The next step is to identify the optimal solution \( \mathbf{x}^* \) of the function \( f \) by solving the gradient equation. \( \nabla f(\mathbf{x}) = \mathbf{0} \). The gradient of \( f \) is given by:
\[
\nabla f(\mathbf{x}) = \begin{bmatrix} x_1 \\ 2x_2 \end{bmatrix}.
\]
The single solution to this system of equations are \( x_1 = 0 \) and \( x_2 = 0 \), thus there is a single optimal solution at the origin \( \mathbf{x}^* = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \).

\subsubsection{Theoretical Convergence Rate}
Now that \(f\) is confirmed to be convex and the Lipschitz constant of the gradient is determined, the following bound for a convex and L-smooth can be used:
$$
    f(\mathbf{x}_K) - f(\mathbf{x}_*) \leq \frac{L \|\mathbf{x}_0 - \mathbf{x}^*\|_2^2}{2K},
$$

Thus, the above inequality can be rearranged to give a bound on the number of iterations $K$ to achieve the desired accuracy. 
\[
K \leq \frac{L \|\mathbf{x}_0 - \mathbf{x}^*\|_2^2}{2(f(\mathbf{x}_K) - f(\mathbf{x}^*))}
\]

Substituting the values of \( L = 2 \), \( \|\mathbf{x}_0 - \mathbf{x}^*\|^2_2 = 2 \) and \(f(\mathbf{x}_K) - f(\mathbf{x}^*) = 0.01 \) into the equation, the number of iterations required to achieve the specified accuracy is calculated as \( K \leq 200 \).

\subsubsection{Practical Convergence Rate}
To practically determine the number of iterations required to achieve the specified accuracy, the gradient descent algorithm is implemented in Python with a learning rate of \( \alpha = \frac{1}{L} = \frac{1}{2} \). The algorithm is run starting at 
\( \mathbf{x}_0 = \begin{bmatrix} 1 \\ 1 \end{bmatrix} \) and iterated until the difference between the function at the current point and the optimal solution is less than 0.01. The number of iterations required to achieve this accuracy is found to be 3, which while consistent with the theoretical prediction, is two orders of magnitude lower.

\subsubsection{Strong Convexity of the Function}
The primary reason for the observed convergence rate being significantly lower than the theoretical bound is that the strong convexity of the function has not considered. A function \( f \) is strongly convex with parameter \( \mu \) if the following holds:
\[
\nabla^2 f(\mathbf{x}) \succeq \mu \mathbf{I} \quad \forall \mathbf{x} \in \mathbb{R}^2,
\]
where \( \nabla^2 f(\mathbf{x}) \) is the Hessian matrix of \( f \) and \( \mathbf{I} \) is the identity matrix. Which is equivalent to stating that \( H_f(\mathbf{x}) - \mu \mathbf{I} \) must be positive semi-definite for all \( \mathbf{x} \in \mathbb{R}^2 \). The Hessian matrix of \( f \) is given by:
\[
H_f(\mathbf{x}) = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix}.
\] The smallest value of \( \mu \) that satisfies this condition is 1.

Now that \( f \) is confirmed to be strongly convex with \( \mu = 1 \), the following bound for a strongly convex and L-smooth function can be used:
% Derive the convergence rate if word count available.

\[
\|\mathbf{x}_K - \mathbf{x}^*\|_2^2 \leq \left(1 - \frac{\mu}{L}\right)^K \|\mathbf{x}_0 - \mathbf{x}^*\|_2^2
\]

This can be seen to give a convergence rate of $\mathcal{O}\left(\frac{1}{\epsilon}\right)$, where $\epsilon$ is the desired accuracy. In this case the convergence rate order evaluates to approximately 5 which is more consistent with the practical results.

\subsection{Part B - Data Driven and Model Driven Reconstruction}
This exercise compares the performance of data-driven and model-driven approaches for solving the image reconstruction inverse problem. Here the goal is to minimise the objective function:
\[
\min_{\mathbf{x}} \left\| \mathbf{y} - \mathbf{A} \mathbf{x} \right\|_2^2 + \lambda \left\| \mathbf \nabla \mathbf{x} \right\|_1,
\]
where in this context \( \mathbf{A} \) is the CT forward operator, \( \mathbf{y} \) is the observed noisy sinogram, \( \mathbf{x} \) is the true image to be reconstructed, and \( \lambda \) is the regularisation parameter. The particular choice of regularisation is known as the total variation (TV) regularisation, which promotes sparsity in the gradient of the image.
Relevant code for this question can be found in the \texttt{src/q3b.py} or \texttt{src/coursework\_LGD\_filled\_vj.ipynb}.
\subsubsection{Model-Driven Approach}
Such a problem would typically be solved using proximal gradient descent however the proximal operator for the total variation regularisation is not straight forward to calculate. However, the proximal operator can be computed under a variable splitting scheme, so the problem is tackled using the alternating direction method of multipliers (ADMM) algorithm. Here a new quantity \( \mathbf{z} = \nabla \mathbf{x}\) is introduced and a penalty is added to the objective function to enforce the constraint \( \mathbf{z} = \nabla \mathbf{x} \). The ADMM algorithm is then used to solve the problem iteratively and can be summarised as follows:

\begin{algorithm}[H]
\caption{Alternating Direction Method of Multipliers (ADMM)}
\begin{algorithmic}[1]
\State \textbf{initialise:} $\mathbf{x}_0, \mathbf{z}_0, \mathbf{u}_0, \rho$
\State \textbf{define:} $L_\rho(\mathbf{x}, \mathbf{z}, \mathbf{u}) = \left\| \mathbf{y} - \mathbf{A} \mathbf{x} \right\|_2^2 + \lambda \left\| \mathbf{z} \right\|_1 + \mathbf{u}^T (\nabla \mathbf{x}-\mathbf{z})+\frac{\rho}{2} \left\| \nabla \mathbf{x} - \mathbf{z}\right\|_2^2$  \Comment{Define the augmented Lagrangian}
\While{not converged}
    % Define lp
    \State $\mathbf{x}_{k+1} \gets \min_x L_\rho(\mathbf{x}, \mathbf{z}_k, \mathbf{u}_k)$ \Comment{Update $x$ by minimizing the augmented Lagrangian}
    \State $\mathbf{z}_{k+1} \gets \min_z L_\rho(\mathbf{x}_{k+1}, \mathbf{z}, \mathbf{u}_k)$ \Comment{Update $z$ via proximal operator of $\|\cdot\|_1$}
    \State $\mathbf{u}_{k+1} \gets \mathbf{u}_k + \rho (\nabla \mathbf{x}_{k+1} - \mathbf{z}_{k+1})$ \Comment{Update the dual variable}
    \State $k \gets k + 1$
\EndWhile
\State \textbf{return} $\mathbf{x}_k, \mathbf{z}_k, \mathbf{u}_k$
\end{algorithmic}
\end{algorithm}

\subsubsection{Data-Driven Approach}
The solution for \( \mathbf{x} \) obtained using the ADMM algorithm is compared to the solution obtained using the data-driven approach of learned gradient descent. The learned gradient descent approach involves replacing the proximal operator at each iteration \(k\) with a neural network \(h_k\) that takes as input the arguments of the proximal operator and returns the output. The algorithm can be summarised as follows:


\begin{algorithm}[H]
    \caption{Learned Gradient Descent}
    \begin{algorithmic}[1]
    \State \textbf{initialize:} $\mathbf{x}_0$, $N$
    \State \textbf{define:} $f(\mathbf{x}) = \left\| \mathbf{y} - \mathbf{A} \mathbf{x} \right\|_2^2$ \Comment{Define the data fidelity term}
    \For{$k = 1$ to $N$}
        \State $\mathbf{g}_k \gets \mathbf{A}^T(\mathbf{A}\mathbf{x}_k - \mathbf{y})$ \Comment{Gradient computation}
        \State $\mathbf{dx} \gets h_k(\mathbf{x}_k, \mathbf{g}_k; \theta_k)$ \Comment{Apply learned proximal mapping}
        \State $\mathbf{x}_{k+1} \gets \mathbf{x}_k + \tau_k \mathbf{dx}$ \Comment{Update the solution}
    \EndFor
    \State \textbf{return} $\mathbf{x}_N$
    \end{algorithmic}
    \end{algorithm}

\( \mathbf{x}_0 \) is set to be the filtered back projection (FBP) reconstruction of the sinogram \( \mathbf{y} \). This provides a reasonable initial estimate for the image reconstruction problem.

\subsubsection{Model Architecture}
The architecture of the unrolling neural network \( h_* \) is a convolutional neural network that processes concatenated tensors of the current iterate $x$ and the gradient $u$. It comprises three convolutional layers, each with a kernel size of $3 \times 3$ and a stride of $1$. The first layer transforms 2 input channels to 32 output channels, followed by a Parametric Rectified Linear Unit (PReLU) activation. The second layer continues with 32 channels, also followed by PReLU. The final layer reduces the channel count from 32 to 1. The output of the network delta $dx$ is multiplied with a learnable step size $\tau_k$ which is added as a corrective term to the current estimate $x_i$ at each iteration.
\subsection{Training}
The neural network is trained in a supervised manner using the Adam optimiser with a learning rate of $10^{-4}$, across 2000 epochs. The learned gradient descent algorithm is executed for 5 iterations, employing mean square error as the loss function, calculated between the final estimate $x_5$ and the true image $x^*$.

\subsection{Results}
The results of the model-driven and data-driven approaches are compared in Figure \ref{fig:reconstruction_comparison}. The data-driven approach is observed to produce a reconstruction with a higher level of detail, confirmed by the higher peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM) compared to the model-driven approach and simple FBP reconstruction. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/q3_2_results.png}
    \caption{Comparison of the FBP, model-driven, and data-driven reconstructions of the image.}
    \label{fig:reconstruction_comparison}
\end{figure}
\section{Appendix}
\subsection{Co-pilot and ChatGPT Usage}
GitHub CoPilot was used heavily to to generate the code in \texttt{src/figs.ipynb}. It was also used to create the docstrings for the functions and classes in this repository. CoPilot was also used to help define and format the figures and tables in this report. Any code from chat gpt is commented as such in the code.

\bibliographystyle{plain}  % Choose the style that suits your needs
\bibliography{references}  % The filename of the .bib file, without the extension
\end{document}